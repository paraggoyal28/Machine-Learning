{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "df = pd.read_csv('gdrive/My Drive/learning/cloth_prediction/train.csv')\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import natsort\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import model_selection\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from zipfile import ZipFile\n",
    "file_name = \"/content/gdrive/My Drive/learning/cloth_prediction/train.zip\"\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DIR = 'training/training'\n",
    "\n",
    "IMG_SIZE=240\n",
    "train_data_240=[]\n",
    "labels_240=[]\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "    path = os.path.join(TRAIN_DIR,img)\n",
    "    img_data = cv2.imread(path)\n",
    "    img_data = cv2.resize(img_data, (IMG_SIZE,IMG_SIZE))\n",
    "    train_data_240.append([np.array(img_data)])\n",
    "    number = int(img[:-4])\n",
    "    categ_num = int(solution[solution.id==number].category)\n",
    "    labels_240.append(categ_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(train_data_240,labels_240,\n",
    "                                                               test_size=0.25,random_state=42)\n",
    "\n",
    "aug=ImageDataGenerator(rotation_range=25,width_shift_range=0.1,height_shift_range=0.1,\n",
    "                       shear_range=0.2,horizontal_flip=True,fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding=\"same\",activation=\"linear\",input_shape=(240,240,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding=\"same\",activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128,(3,3),padding=\"same\",activation=\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"linear\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "#reshape it\n",
    "# x_train = x_train/255.0\n",
    "# x_test = x_test/255.0\n",
    "\n",
    "train=model.fit_generator(aug.flow(x_train,y_train,batch_size=64),validation_data=(x_test,y_test),\n",
    "                          steps_per_epoch=len(x_train)/64,epochs=150,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save this data, so that we don't need to keep calculating it every\n",
    "#time we want to play with the neural network model:\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "\n",
    "#We can always load it in to our current script, or a totally new one by doing:\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all lines\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Pclass.value_counts(normalize=True).plot(kind=\"bar\")\n",
    "\n",
    "\n",
    "plt.scatter(df.Survived,df.Age, alpha=0.5)\n",
    "\n",
    "\n",
    "for x in [1,2,3]:\n",
    "    df.Age[df.Pclass == x].plot(kind=\"kde\")\n",
    "plt.legend((\"1st\", \"2nd\", \"3rd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns\n",
    "import pandas as pd\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1, 2, 0.3) \n",
    "# >>  [1.  1.3 1.6 1.9]\n",
    "\n",
    "np.linspace(1, 2, 7)\n",
    "# >>  [1. 1.16666667 1.33333333 1.5  1.66666667 1.83333333     2.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_sum = lambda x, y: x + y\n",
    "reduce(lambda_sum, [1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check NULL values\n",
    "df.isnull().values.any()\n",
    "\n",
    "#check NULL values count\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "temp = df.copy()\n",
    "temp.iloc[:,1] = label_encoder.fit_transform(df.iloc[:,1])    #1 is column index\n",
    "temp.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "standard_scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "print(standard_scaler.fit_transform(X))\n",
    "print(normalizer.fit_transform(X))\n",
    "print(min_max_scaler.fit_transform(X))\n",
    "\n",
    "\n",
    "# One hot encoding\n",
    "pd.get_dummies(df.iloc[:,1])\n",
    "temp = pd.concat([temp,onehot], axis = 1)\n",
    "temp = temp.drop(labels = [\"Tag\"],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Linear Regression with summary\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "feature_cols = ['Reputation',  'Views', 'tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6', 'tag_7', 'tag_8', 'tag_9']\n",
    "\n",
    "X = temp[feature_cols]\n",
    "y = temp.Upvotes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "model1=sm.OLS(y_train,X_train)\n",
    "result=model1.fit()\n",
    "\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#linear regressio with gradient descent\n",
    "\n",
    "def linear_regression(X, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001):\n",
    "    N = float(len(y))\n",
    "    for i in range(epochs):  \n",
    "        y_current = (m_current * X) + b_current\n",
    "        cost = sum([data**2 for data in (y-y_current)]) / N\n",
    "        m_gradient = -(2/N) * sum(X * (y - y_current))\n",
    "        b_gradient = -(2/N) * sum(y - y_current)\n",
    "        m_current = m_current - (learning_rate * m_gradient)\n",
    "        b_current = b_current - (learning_rate * b_gradient)\n",
    "    return m_current, b_current, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mean square error and r2_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_predict = slr.predict(X_test) # predict the Y based on the model\n",
    "mean_squared_error = mean_squared_error(y_test,y_predict) # calculate mean square error\n",
    "r2_score = r2_score(y_test,y_predict) #calculate r square\n",
    "\n",
    "print ('mean square error:',mean_squared_error )\n",
    "print ('r square:',r2_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "# calculate AUC\n",
    "print metrics.roc_auc_score(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.2.16\n",
    "!pip install opencv-contrib-python==3.4.2.16\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "face_cascade.load('/usr/local/lib/python3.6/dist-packages/cv2/datahaarcascades/haarcascade_frontalface_default.xml')\n",
    "print(face_cascade.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ = cv2.imread('train/'+os.listdir('train')[4])\n",
    "##img1 = cv2.cvtColor(img_,cv2.COLOR_BGR2GRAY)\n",
    "img1 = cv2.cvtColor(img_,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "## cv2.cvtColor convert the input rgb image into its grayscale form \n",
    "\n",
    "img = cv2.imread('train/'+os.listdir('train')[502])\n",
    "img2 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the p-values for the model coefficients\n",
    "lm1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You see number of unique item for Species with command below:\n",
    "dataset['Species'].unique()\n",
    "\n",
    "# grouping\n",
    "dataset.groupby('Species').count()\n",
    "\n",
    "#where\n",
    "dataset.where(dataset ['Species']=='Iris-setosa') #or\n",
    "dataset[dataset['SepalLengthCm']>7.2]\n",
    "\n",
    "# Seperating the data into dependent and independent variables\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data according to dependent and in dependent variable\n",
    "X = iris.iloc[:, :-1].values\n",
    "y = iris.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scalers\n",
    "# Standard scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "\n",
    "#min-max Scalar\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Robust scaler\n",
    "scaler = preprocessing.RobustScaler()\n",
    "\n",
    "#normalizer\n",
    "scaler = preprocessing.Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot in sns\n",
    "\n",
    "# Modify the graph above by assigning each species an individual color.\n",
    "sns.FacetGrid(dataset, hue=\"Species\", size=5) \\\n",
    "   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "   .add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Spelling Corrector\n",
    "\n",
    "\n",
    "import re, collections\n",
    "\n",
    "def tokens(text): \n",
    "    \"\"\"\n",
    "    Get all words from the corpus\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "WORDS = tokens(file('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "# top 10 words in corpus\n",
    "print WORD_COUNTS.most_common(10)\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually \n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}\n",
    "\n",
    "\n",
    "def edits0(word): \n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away \n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}\n",
    "\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs \n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) \n",
    "                for i in range(len(word)+1)]\n",
    "                \n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "    \n",
    "    \n",
    "def correct(word):\n",
    "    \"\"\"\n",
    "    Get the best correct spelling for the input word\n",
    "    \"\"\"\n",
    "    # Priority is for edit distance 0, then 1, then 2\n",
    "    # else defaults to the input word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "\n",
    "def correct_match(match):\n",
    "    \"\"\"\n",
    "    Spell-correct word in match, \n",
    "    and preserve proper upper/lower/title case.\n",
    "    \"\"\"\n",
    "    \n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        \"\"\"\n",
    "        Return the case-function appropriate \n",
    "        for text: upper, lower, title, or just str.:\n",
    "            \"\"\"\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "    \n",
    "def correct_text_generic(text):\n",
    "    \"\"\"\n",
    "    Correct all the words within a text, \n",
    "    returning the corrected text.\n",
    "    \"\"\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "\n",
    "\n",
    "print correct_text_generic('fianlly')\n",
    "\n",
    "\n",
    "from pattern.en import suggest\n",
    "\n",
    "print suggest('fianlly')\n",
    "print suggest('flaot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion using label encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "clean_df = train_set.copy()\n",
    "\n",
    "for f in clean_df.columns:\n",
    "    if clean_df[f].dtype == 'object':\n",
    "        label = preprocessing.LabelEncoder()\n",
    "        label.fit(list(clean_df[f].values))\n",
    "        clean_df[f] = label.transform(list(clean_df[f].values))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "GradientBoostingClassifier(criterion=’friedman_mse’, init=None, learning_rate=0.1, loss=’deviance’, max_depth=3,\n",
    "max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2,\n",
    "min_weight_fraction_leaf=0.0, n_estimators=100, presort=’auto’, random_state=None, subsample=1.0,\n",
    "verbose=0, warm_start=False)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "roc_auc\n",
    "\n",
    "\n",
    "> 0.79286211093722836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing text with @\n",
    "\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi.head()\n",
    "\n",
    "#Removing Punctuations, Numbers, and Special Characters\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "combi.head(10)\n",
    "\n",
    "#Removing Short Words\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#Text Normalization\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "\n",
    "# normalization of tokenised or stemming\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
