{
  "cells": [
    {
      "metadata": {
        "_uuid": "ab093eb85e92acd8841e4460f6958b058c284efa"
      },
      "cell_type": "markdown",
      "source": "# Beginner Challenge : House prices\n*[TEAM - EDA] - 2018-10-03 ~ 2018-11-14*\n\n*TEAM - EDA is Club in Hanyang Univ. The purpose of this kernel is to participate in the beginnerchallenge in kaggle university club.*\n\n*Member : Hyun woo Kim, ju yeon Park, ji ye lee, ju yeong lee, eun joo min, su min song*\n\n*There is still a little missing part because it is still incomplete.*\n![](https://kaggle2.blob.core.windows.net/competitions/kaggle/5407/media/housesbanner.png)"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## 0. Overview\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Overview.PNG?raw=true)"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "## 1. EDA\n### 1.1 Problem Definition \n\nThe purpose of this competition is to accurately predict the price of a house called `SalePrice`.\nTo do this, we use the evaluation metric called RMSLE.\n![](https://t1.daumcdn.net/cfile/tistory/998C4F405BE420F004)\n\nwhat is RMSLE ?\n\nRMSLE is a way to compare differences after log predicted values and log actual values. The reason for putting the Log without using RMSE is that the range of the objective variable value is wide.\n\n\n#### Competition description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa (central Iowa in America) , this competition challenges you to predict the final price of each home.\n\nNow, we have to do data exploration"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6de49834f0d3d79082c9e7b6b7338a49466a4715"
      },
      "cell_type": "code",
      "source": "# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f8cdd6c35a467026ec197c7f2b0f5b035ebbc10"
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv('../input/train.csv')\ndf_test  = pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a1af250d33778269797a041cb51e88ea38f19fb"
      },
      "cell_type": "code",
      "source": "print(\"train.csv. Shape: \",df_train.shape)\nprint(\"test.csv. Shape: \",df_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee2871f04e141e7ec9d97b3720f09b957b2e2ba4"
      },
      "cell_type": "markdown",
      "source": "Train and Test sizes are very similar to 1460 and 1459. And there are so many variables. \nSo we have to choose the feature selection and cross validation strategy. "
    },
    {
      "metadata": {
        "_uuid": "59c78112ff6dcacdf46b5a09d144e9cde7a7610a"
      },
      "cell_type": "markdown",
      "source": "Next, we read the Data Description.txt (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) and make a excel file like below picture\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/variable%20description.PNG?raw=true)\n\n\nThrough this process, you can see five things.\n- What is the definition of a variable ?\n- What the value of a variable has ?\n- How many unique values of a variable ?\n- As in the NA description above Table, there are variables whose NA means None.\n-  As shown in the circle graph, the categorical variables are mixed in the int variables. Among them, there are 20 ordinal variables.\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Data%20Type.PNG?raw=true)"
    },
    {
      "metadata": {
        "_uuid": "99d6d8526c2116e394cb5a44c2e164d7db3b184c"
      },
      "cell_type": "markdown",
      "source": "### 1.2 Visualization : SalePrice (Target Value)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42601d44e5ed16454561f14d98d8bc459d48e12f"
      },
      "cell_type": "code",
      "source": "#descriptive statistics summary\ndf_train['SalePrice'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d385063e7ef0fab6080e9e97aee66c480d142098"
      },
      "cell_type": "markdown",
      "source": "  - The std is big.\n  - min is greater than 0\n  - There is a big difference between the minimum value and the 25th percentile.\n  - It's bigger than the 75th percentile and max.\n  - The difference between the 75th percentile and the max is greater than the 25th percentile and the max."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d14e33bec97d8d93c73d210efacafd17f7099e3"
      },
      "cell_type": "code",
      "source": "#histogram\nf, ax = plt.subplots(figsize=(8, 6))\nsns.distplot(df_train['SalePrice'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60d281209e5e7a934d53b486077322563751bcee"
      },
      "cell_type": "markdown",
      "source": "- Long tail formation to the right (not normal distribution)\n    - Q. Should I normalize? \n\n    - A. Yes. There are two purposes for normalization here. 1. To make the same status evaluation metric as the  RMSLE. 2. To satisfy the assumption of linear regression."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acdcce30058dca201e9053a14e14e73ab335021d"
      },
      "cell_type": "code",
      "source": "#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4727eed06d12b2120ef6c9215d405781acbabd28"
      },
      "cell_type": "markdown",
      "source": "- Skewness: The longer the right tail, the more positive the tail\n- Kurtosis (kurtosis / kurtosis): If the kurtosis value (K) is close to 3, the scatter is close to the normal distribution. (K <3), the distributions can be judged to be flattened more smoothly than the normal distribution, and if the kurtosis is a positive number larger than 3 (K> 3), the distribution can be considered to be a more pointed distribution than the normal distribution\n\n- Long tail formation to the right (not normal distribution)\n    - Q. Should I normalize? \n\n    - A. Yes. There are two purposes for normalization here. 1. To make the same status evaluation metric as the  RMSLE. 2. To satisfy the assumption of linear regression."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "622d6687b8be3b38a59520534adfa79363e758dc"
      },
      "cell_type": "code",
      "source": "fig = plt.figure(figsize = (15,10))\n\nfig.add_subplot(1,2,1)\nres = stats.probplot(df_train['SalePrice'], plot=plt)\n\nfig.add_subplot(1,2,2)\nres = stats.probplot(np.log1p(df_train['SalePrice']), plot=plt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1053e23d95ce182d15734252abc5c2f933553dd4"
      },
      "cell_type": "code",
      "source": "df_train['SalePrice'] = np.log1p(df_train['SalePrice'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4de4e22cafbd4ea85d467106e129d1deb619ea69"
      },
      "cell_type": "code",
      "source": "df_train['SalePrice'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "33fc0cf63bad9427ed379e120196dcdd05fde421"
      },
      "cell_type": "markdown",
      "source": "### 1.2 Visualization : High correlated variable with SalePrice & 1.3 Find Na, Missing values\n\n- `Pearson product ratio correlation` : \nPearson correlation evaluates the linear relationship between two metric variables. There is a linear relationship when the variation of one variable is proportional to the change of another variable. For example, Pearson correlation can be used to assess whether the increase in temperature in a production facility is related to changes in the thickness of the chocolate coating.\n\n- `Spearman Rank Correlation` : \nSpearman correlation evaluates the simple relationship between two metric or sequential variables. In a simple relationship, the two variables tend to change together, but not necessarily at a constant rate. The Spearman correlation coefficient is based on the ranked value for each variable, not the raw data. Spearman correlation is often used to evaluate relationships containing sequential variables. For example, you can use Spearman correlation to assess whether the order in which employees complete the test exercises is related to the number of months employed.\n\n![](https://choco9966.github.io/Team-EDA//image/correlation1.png)"
    },
    {
      "metadata": {
        "_uuid": "de244b69a5dcc91326ac9d11fa29cc6383652579"
      },
      "cell_type": "markdown",
      "source": "               Pearson = +1, Spearman = +1                 Pearson = +0.851, Spearman = +1"
    },
    {
      "metadata": {
        "_uuid": "091d173f94a238d7f0caf62ef7002b0f83255563"
      },
      "cell_type": "markdown",
      "source": "so  we must use spearman correlation, becuase categorical variables are mixed, "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "058817a2f4f492b943988f649f7317b055178d4a"
      },
      "cell_type": "code",
      "source": "#saleprice correlation matrix\nk = 15 #number of variables for heatmap\ncorrmat = abs(df_train.corr(method='spearman')) # correlation 전체 변수에 대해서 계산\ncols = corrmat.nlargest(k, 'SalePrice').index # nlargest : Return this many descending sorted values\ncm = np.corrcoef(df_train[cols].values.T) # correlation 특정 컬럼에 대해서\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(8, 6))\n\n#Sokratis Kouvaras help to hide upper symmetric metrics\nmask = np.zeros_like(cm) \nmask[np.triu_indices_from(mask)] = True \nsns.set_style(\"white\")\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values, mask = mask)\nplt.show()\n#hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72b849f920aa7821aa79f0f3504c1990aad40db3"
      },
      "cell_type": "markdown",
      "source": "9 most relevant variables with SalePrice\n- OverallQual : Overall material and finish quality\n- GrLivArea : Above grade (ground : the portion of a home that is above the ground) living area square feet\n- GarageCars : Size of garage in car capacity\n- GarageArea : Size of garage in square feet\n- TotalBsmtSF : Total square feet of basement area (지하실 the lowermost portion of a structure partly or wholly below ground level; often used for storage)\n- 1stFlrSF : First Floor square feet\n- FullBath : Full bathrooms above grade\n- TotRmsAbvGrd : Total rooms above grade (does not include bathrooms)\n- YearBuilt : Original construction date"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2f5793bfec5320dc3e385e4afe43b8dda0fa811"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "426b2124ce61c5c9543cd19585c1b752dd1c5c52"
      },
      "cell_type": "markdown",
      "source": "- The higher the quality, the better the selling price.\n- However, the 3rd, 4th level of outliers and 7 and 10 outliers do not have anything suspicious. These values should be checked again later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ee5daf4285a815bf5e8c82e431f40ae75385c04"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='GrLivArea', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b5912d75649c0dbe735f049abe47e07dc88f769"
      },
      "cell_type": "markdown",
      "source": "The relationship between GrLivArea and SalePrice has a positive correlation. That is, as the area becomes wider, the price also increases. However, irrespective of that, the GrLivArea > 4000 and SalePrice < 13 seems outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa5109e2d2f2facd22c067c1493bc09a075ef924"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['GarageCars']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='GarageCars', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d049aaaf85f869833226e1121df4fa9832b36477"
      },
      "cell_type": "markdown",
      "source": "- GarageCars 4 is very strange ... why? we find the reason by EDA"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1466b13b79ab161772da17df76ffab0c76b42663"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='YearBuilt', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "032421417492590c06f3ea05fb64602d22867855"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['GarageArea']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='GarageArea', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b01c9ebdb2f210d702d5874323c482c356bba7dc"
      },
      "cell_type": "markdown",
      "source": "- GarageArea is divided into zero and non-zero parts.\n- Generally, there is a positive correlation. But some points seem to outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad3f06d8254bc2c7dd07be728064ff1b673dc4f2"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='TotalBsmtSF', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3eac15de2ed52bb1e17d7095a9cc411eb03f5958"
      },
      "cell_type": "markdown",
      "source": "- The point of TotalBsmtSF >6000 seems outlier"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f37876c60b9de37f502d18356096bf891d85df1"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['FullBath']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='FullBath', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e4797df43cba824668c2eea375c3accb9b8b957"
      },
      "cell_type": "markdown",
      "source": "It is strange that FullBath is zero higher. But if you read the Data Description above, you can see that there are many variables related to Bath like halfBath etc..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03bd683c053d8715b3ce518aeacdadf04b6bcba7"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['TotRmsAbvGrd']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='TotRmsAbvGrd', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e21fbf2be0a2aad71aab7a44f6f7a780b84da10"
      },
      "cell_type": "markdown",
      "source": "I think that outlier\n\n- Separate analysis only for 12, 14\n- Very low values in 6\n- very high value at 10"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2dcd0ebbbe55c942303968642dfcaccd5f127709"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['1stFlrSF']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='1stFlrSF', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b21a7bc643a068ce3aacba1bfa77df947d58d63e"
      },
      "cell_type": "code",
      "source": "data = pd.concat([df_train['SalePrice'], df_train['LotArea']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='LotArea', y=\"SalePrice\", data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4745b2ebfe375face05fab41a144f2505c201359"
      },
      "cell_type": "markdown",
      "source": "### 1.2 Visualization : Object Variable \nI referenced the code in the link . [Samarth Agrawal EDA for Categorical Variables - A Beginner's Way\n](https://www.kaggle.com/nextbigwhat/eda-for-categorical-variables-a-beginner-s-way)\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a9a9d34454ae68ad5f02aadd86bd8487ec1f0f3"
      },
      "cell_type": "code",
      "source": "categorical_features = df_train.select_dtypes(include = [\"object\"]).columns\n\nix = 1\nfig = plt.figure(figsize = (15,10))\nfor c in list(df_train[categorical_features]):\n    if ix <= 3:\n            ax2 = fig.add_subplot(2,3,ix+3)\n            sns.boxplot(data=df_train, x=c, y='SalePrice', ax=ax2)\n            #sns.violinplot(data=ds_cat, x=c, y='SalePrice', ax=ax2)\n            #sns.swarmplot(data = ds_cat, x=c, y ='SalePrice', color = 'k', alpha = 0.4, ax=ax2)\n            \n    ix = ix +1\n    if ix == 4: \n        fig = plt.figure(figsize = (15,10))\n        ix =1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4fd1ca7a902b4b48fbf46cc140798f43e1d0fa79"
      },
      "cell_type": "markdown",
      "source": "We can see which variables have a large effect on SalePrice like PoolQC, KitchenQual, Condition1 and 2, Neighborhood, Alley, MSZoning "
    },
    {
      "metadata": {
        "_uuid": "327ffc5cd2b7943ad6abd36bee3ec220e016e269"
      },
      "cell_type": "markdown",
      "source": "### 1.3 Finding : Missing Values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "530fe47f0e404074158e2efc86f1b13448ac61bb"
      },
      "cell_type": "code",
      "source": "#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#histogram\n#missing_data = missing_data.head(20)\npercent_data = percent.head(20)\npercent_data.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"Count\", fontsize = 20)\nplt.title(\"Total Missing Value (%)\", fontsize = 20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b8c3895a9ba5def0057a39a463253582f24c5f5"
      },
      "cell_type": "markdown",
      "source": "we check 2 point. \n- Why was there a missing vales?\n- How should I handle missing values?\n\nHowever, the above process is not an easy process and requires much research."
    },
    {
      "metadata": {
        "_uuid": "114eca367b801c0e947f22029d5b09633eac08ca"
      },
      "cell_type": "markdown",
      "source": "### 1.4 Train vs Test"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1d5cb35af0ecd5e38e2d9a5cad8d05f97f91a50"
      },
      "cell_type": "code",
      "source": "Numeric = df_train.copy()\ndel Numeric['SalePrice']\nNumeric_columns = Numeric.select_dtypes(include = [\"int64\",\"float64\"]).columns\n\nix = 1\nfig = plt.figure(figsize = (15,10))\nfor c in list(Numeric_columns):\n    if ix <= 3:\n            ax2 = fig.add_subplot(2,3,ix+3)\n            sns.distplot(df_train[c].dropna())\n            sns.distplot(df_test[c].dropna())\n            plt.legend(['train', 'test'])\n            plt.grid()            \n    ix = ix +1\n    if ix == 4: \n        fig = plt.figure(figsize = (15,10))\n        ix =1\ndel Numeric",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d5f269833635f239119ad5c791303f466665ff2"
      },
      "cell_type": "markdown",
      "source": "almost they have similiar distribution between train and test"
    },
    {
      "metadata": {
        "_uuid": "674329fa4ede71d5b11c1df2487b9beadbec3696"
      },
      "cell_type": "markdown",
      "source": "## 2. Preprocessing\n### 2.1 Data Cleaning : Outliers"
    },
    {
      "metadata": {
        "_uuid": "67fc92d5b44943a096f8b3ea058d8727ee444258"
      },
      "cell_type": "markdown",
      "source": "In fact, it is difficult to judge whether it is an outlier or not by only the above information. And since the number of data is as small as 1400, the removal of outlier affects the model greatly. The easiest way to do this is to remove the values we thought were outliers and to see if the leader board score (LB) and cross validation score (CV) were going up."
    },
    {
      "metadata": {
        "_uuid": "3a7a34449e3a0befaa3b5cd386a5bba9301199bb"
      },
      "cell_type": "markdown",
      "source": "Through such a process, I decided on outliers below."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3617fd11a47884853edd2877d2dcd7cac8612230"
      },
      "cell_type": "code",
      "source": "# Deleting outliers\ndf_train = df_train[df_train['Id'] != 692][df_train['Id'] != 1183]\n\n# drop outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<12.5)].index)\ndf_train = df_train.drop(df_train[(df_train['LotArea']>150000)].index)\ndf_train = df_train.drop(df_train[(df_train['GarageArea']>1200) & (df_train['SalePrice']<12.5)].index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7597b81c7182426cc807829b4c802d4904211ee"
      },
      "cell_type": "markdown",
      "source": "### 2.1 Data Cleaning : Missing values\nI refer to the following kernel. [Jared Wang Easy prediction using lightgbm model\n](https://www.kaggle.com/jens0306/easy-prediction-using-lightgbm-model)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42b40f3696b23227f8b190d908b89b93cce8bdbb"
      },
      "cell_type": "code",
      "source": "#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#histogram\n#missing_data = missing_data.head(20)\npercent_data = percent.head(20)\npercent_data.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"Count\", fontsize = 20)\nplt.title(\"Total Missing Value (%)\", fontsize = 20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a747bc896b952956259f4caed0873712be10880e"
      },
      "cell_type": "code",
      "source": "import missingno as msno\nlen_train = df_train.shape[0]\ny_reg = df_train['SalePrice']\nId = df_test['Id']\ndf_all = pd.concat([df_train,df_test])\ndel df_all['Id']\nmissingdata_df = df_all.columns[df_all.isnull().any()].tolist()\nmsno.heatmap(df_all[missingdata_df], figsize=(20,20))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8005acfb202eafbafaa21dccb5f9b1d20c8f1655"
      },
      "cell_type": "markdown",
      "source": "We can look at some variables that have correlations between missing values. like Garage ~, Bsmt ~.\n\n"
    },
    {
      "metadata": {
        "_uuid": "be0285745c755f754e2f22d867d842aef7c31e43"
      },
      "cell_type": "markdown",
      "source": "First, i delete utilities bacause of the value has 2 unique values and Allpubhas freq 2906 , NoSeWa freq 1. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03f89e264a74231bc7fa887a09e0df00f47916dc"
      },
      "cell_type": "code",
      "source": "df_all['Utilities'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e36a33aa603089c9decbf4395cada66aacaad3d3"
      },
      "cell_type": "code",
      "source": "df_all['Utilities'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99ebb5664757867c246ea607104b5bb3788c351f"
      },
      "cell_type": "code",
      "source": "del df_all['Utilities']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e631d74293d9aec8edd7d87f31a576392032721"
      },
      "cell_type": "markdown",
      "source": "And treats the missing values as None like BsmtCond variable\n\n*BsmtCond Values*\n```\n   Ex   Excellent\n   Gd   Good\n   TA   Typical - slight dampness allowed\n   Fa   Fair - dampness or some cracking or settling\n   Po   Poor - Severe cracking, settling, or wetness\n   NA   No Basement\n   ```"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1eba6134e6e467c261e27b86e5543c870b779ee1"
      },
      "cell_type": "code",
      "source": "#in these features, NAN means none\ndf_all[\"PoolQC\"] = df_all[\"PoolQC\"].fillna(\"None\")\ndf_all[\"MiscFeature\"] = df_all[\"MiscFeature\"].fillna(\"None\")\ndf_all[\"Alley\"] = df_all[\"Alley\"].fillna(\"None\")\ndf_all[\"Fence\"] = df_all[\"Fence\"].fillna(\"None\")\ndf_all[\"FireplaceQu\"] = df_all[\"FireplaceQu\"].fillna(\"None\")\ndf_all['BsmtQual'] = df_all['BsmtQual'].fillna('None')\ndf_all['BsmtCond'] = df_all['BsmtCond'].fillna('None')\ndf_all['BsmtExposure'] = df_all['BsmtExposure'].fillna('None')\ndf_all['BsmtFinType1'] = df_all['BsmtFinType1'].fillna('None')\ndf_all['BsmtFinType2'] = df_all['BsmtFinType2'].fillna('None')\ndf_all['GarageType'] = df_all['GarageType'].fillna('None')\ndf_all['GarageFinish'] = df_all['GarageFinish'].fillna('None')\ndf_all['GarageQual'] = df_all['GarageQual'].fillna('None')\ndf_all['GarageCond'] = df_all['GarageCond'].fillna('None')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "309307657d6614995928622db09e1c03af9a9f9d"
      },
      "cell_type": "markdown",
      "source": "And treats the missing values as 0 like BsmtFinSF1 variable\n\n*BsmtFinSF1 Values :  Type 1 finished square feet*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41a76e809fddca62f76e08dc26bbd48443de838d"
      },
      "cell_type": "code",
      "source": "#in these features, NAN means 0\ndf_all['BsmtFinSF1'] = df_all['BsmtFinSF1'].fillna(0)\ndf_all['BsmtFinSF2'] = df_all['BsmtFinSF2'].fillna(0)\ndf_all['BsmtUnfSF'] = df_all['BsmtUnfSF'].fillna(0)\ndf_all['TotalBsmtSF'] = df_all['TotalBsmtSF'].fillna(0)\ndf_all['BsmtFullBath'] = df_all['BsmtFullBath'].fillna(0)\ndf_all['BsmtHalfBath'] = df_all['BsmtHalfBath'].fillna(0)\ndf_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(0)\ndf_all['GarageYrBlt'] = df_all['GarageYrBlt'].fillna(0)\ndf_all['GarageCars'] = df_all['GarageCars'].fillna(0)\ndf_all['GarageArea'] = df_all['GarageArea'].fillna(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62ca4879146ba0bf587c4652d3b5951b34e33af5"
      },
      "cell_type": "markdown",
      "source": "Filling a missing value with simply mode is a risk of Bias. But it is the most representative method."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b4b1ea48eb7c54e0db093161d5d27297d0b72d1"
      },
      "cell_type": "code",
      "source": "# These features, we just fill them with common case\ndf_all['MSZoning'] = df_all['MSZoning'].fillna(df_all['MSZoning'].mode()[0])\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\ndf_all['MasVnrType'] = df_all['MasVnrType'].fillna(df_all['MasVnrType'].mode()[0])\ndf_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])\ndf_all['Functional'] = df_all['Functional'].fillna(df_all['Functional'].mode()[0])\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd98560ac11b1c391c96d105e20422d3d2f98c60"
      },
      "cell_type": "markdown",
      "source": "*LotFrontage : Linear feet of street connected to property*\n\nWe will fill LotFrontage in assuming that LotFrontage is similar between neighbors. \nSo we have to make sure that the assumption is correct.\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Lotfrontage.PNG?raw=true)"
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "trusted": true,
        "_uuid": "dce7ec889edc45eb872d225e00618421b71ac5d4"
      },
      "cell_type": "code",
      "source": "df_all[df_all['Neighborhood']=='BrkSide']['LotFrontage'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bff3793aed15a6467b7876bafb67970bf28675b"
      },
      "cell_type": "code",
      "source": "df_all[df_all['Neighborhood']=='CollgCr']['LotFrontage'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "57f9dcb9331afcd660a8ba80e769cffafffb3706"
      },
      "cell_type": "markdown",
      "source": "When I look at two situations, it is similar from 25quantile to 75quantile, so it may be ok to fill with median"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9492be5b5fcc8b035e96095696e2e8af6b6e1aa7"
      },
      "cell_type": "code",
      "source": "# LotFrontage has more missing value, thus we consider it more delicately\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e3750a1ec897175277fe93f4172a0033348e1b5"
      },
      "cell_type": "code",
      "source": "#missing data\ntotal = df_all.isnull().sum().sort_values(ascending=False)\npercent = (df_all.isnull().sum()/df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#histogram\n#missing_data = missing_data.head(20)\npercent_data = percent.head(20)\npercent_data.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"Count\", fontsize = 20)\nplt.title(\"Total Missing Value (%)\", fontsize = 20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc00f0d5ea183041cc00b299b81ee89eb11ef74c"
      },
      "cell_type": "markdown",
      "source": "Now, all variables has no NA"
    },
    {
      "metadata": {
        "_uuid": "74b4c3089a3f8bcdeb39fe7d6da44393f6937880"
      },
      "cell_type": "markdown",
      "source": "### 2.3 Object Encoding : one-hot, Label, Frequency\n*2.2 Normalization is done*\n\nWe need to encoding object type data so it can be read by a computer. Typically, one-hot, label, and frequency are used, and how to use it will be judged by the performance of LB and CV.\n\n#### 2.3.1. One-hot Encoding\n*Example*\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/one-hot.PNG?raw=true)"
    },
    {
      "metadata": {
        "_uuid": "d13416a4d65d037dcf1834f4bdf16b7be9682eb5"
      },
      "cell_type": "markdown",
      "source": " Changing OverallCond,MSSubClass into a categorical variable"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ebc121b8486ebe02b9a26c5e34e073eabb523a9"
      },
      "cell_type": "code",
      "source": "df_all['MSSubClass'] = df_all['MSSubClass'].apply(str)\n\n# Changing OverallCond into a categorical variable\ndf_all['OverallCond'] = df_all['OverallCond'].astype(str)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab31b2b16d781335cba4e3deb9a6225cb35c955b"
      },
      "cell_type": "code",
      "source": "categorical_features = df_all.select_dtypes(include = [\"object\"]).columns\nnumerical_features = df_all.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6a81a604bea7d3092b07be3c7cbfd5d4f581998"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\none_hot_encoding = df_all.copy()\none_hot_encoding = pd.get_dummies(one_hot_encoding)\n#len_train\none_hot_encoding.iloc[:,36:50].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "180c04fc0a2e81a70071c7f500f617042fdb76ad",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "del one_hot_encoding;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a9ea6bf7b3f7f11ecf49ad1cfd0f6677b92a0f77"
      },
      "cell_type": "markdown",
      "source": "#### 2.3.2. Label Encoding\n*Example*\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Label.PNG?raw=true)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecc66f485e33ea57ee2a087edb62ec8456c6353b"
      },
      "cell_type": "code",
      "source": "label_encoding = df_all.copy()\nfor i in categorical_features:\n    label_encoding[i], indexer = pd.factorize(label_encoding[i])\nlabel_encoding.iloc[:,20:30].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "471d81cdfcaf8763e6b6a279ed76a7446bd9d10b"
      },
      "cell_type": "code",
      "source": "del label_encoding;",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a911e4cb273b505fae0ebe406621c0a1246b510"
      },
      "cell_type": "markdown",
      "source": "#### 2.3.3. Frequency Encoding\n*Example*\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Frequency.PNG?raw=true)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0aca0964db55a8cff40ce9f33bed512d5f55a7db"
      },
      "cell_type": "code",
      "source": "frequency_encoding_all = df_all.copy()\n    \ndef frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')\n\nfor col in categorical_features:\n    frequency_encoding_all = frequency_encoding(frequency_encoding_all, col)\nfrequency_encoding_all = frequency_encoding_all.drop(categorical_features,axis=1, inplace=False)\nfrequency_encoding_all.iloc[:,20:30].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "c1ec0fb04846dbedcf9a0bcb103d1777dfb21047"
      },
      "cell_type": "code",
      "source": "del frequency_encoding_all",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d3791c2f8cb4ae1225bfcae1b66b9f1769b0a2a5"
      },
      "cell_type": "markdown",
      "source": "### 2.4. Cross Validation Strategy\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Cross%20validation.PNG?raw=true)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e96991589cfa23cac197abc4f49c687de8a9b3f4"
      },
      "cell_type": "markdown",
      "source": "Cross validation is very important. It is a criterion to judge the performance of the model in the competition. Because LB uses only 50% of the total data (usually 20%), it can secure the reliability of the model. \n\nThe hold out method is used when there is no difference between the CVs of folds or data related time. So i use the k-fold or LOOCV  by testing encoding method"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48c8d3c4280caaef241b879fc92b4201bae1eef7"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n#K-folds Validation function\nn_folds = 5 # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "19233c2d6450a3e7b8ab4c4c04361dc994dcca8c"
      },
      "cell_type": "code",
      "source": "one_hot_encoding = df_all.copy()\ndel one_hot_encoding['SalePrice']\none_hot_encoding = pd.get_dummies(one_hot_encoding)\none_hot_encoding_train = one_hot_encoding[:len_train]\none_hot_encoding_test = one_hot_encoding[len_train:]\ndel one_hot_encoding\n\nimport lightgbm as lgb\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \nscore = rmsle_cv(model,one_hot_encoding_train)  \nprint(\"One-hot encoding(5-folds) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nkfold_lgb_model = model.fit(one_hot_encoding_train, y_reg)\ntrain_prediction = model.predict(one_hot_encoding_train)\nprediction = np.expm1(kfold_lgb_model.predict(one_hot_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('OH_5FOLD.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2227ca689b17a0842636a4d2417a4d21ee44d7c8",
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "#K-folds Validation function\nn_folds = one_hot_encoding_train.shape[0] # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "trusted": true,
        "_uuid": "7afa5b8d218163f111fd00872f22fcd5aa04e616"
      },
      "cell_type": "code",
      "source": "import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \nscore = rmsle_cv(model,one_hot_encoding_train)  \nprint(\"One-hot encoding(LOOCV) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nloovc_lgb_model = model.fit(one_hot_encoding_train, y_reg)\ntrain_prediction = model.predict(one_hot_encoding_train)\nprediction = np.expm1(loovc_lgb_model.predict(one_hot_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('OH_LOOCV.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "acbab2d9c65d55bff3673275cffc0ae4ff2397aa"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n#K-folds Validation function\nn_folds = 5 # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nlabel_encoding = df_all.copy()\nfor i in categorical_features:\n    label_encoding[i], indexer = pd.factorize(label_encoding[i])\n    \ndel label_encoding['SalePrice']\nlabel_encoding_train = label_encoding[:len_train]\nlabel_encoding_test = label_encoding[len_train:]\ndel label_encoding\n\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \n\nscore = rmsle_cv(model,label_encoding_train)  \nprint(\"Label encoding(5-folds) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nkfold_lgb_model = model.fit(label_encoding_train, y_reg)\ntrain_prediction = model.predict(label_encoding_train)\nprediction = np.expm1(model.predict(label_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('Label_5FOLD.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "c83031de1d5dc505caa6ef216a7487ef3a302ee1"
      },
      "cell_type": "code",
      "source": "#K-folds Validation function\nn_folds = label_encoding_train.shape[0] # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \nscore = rmsle_cv(model,label_encoding_train)  \nprint(\"Label encoding(LOOCV) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nloovc_lgb_model = model.fit(label_encoding_train, y_reg)\ntrain_prediction = model.predict(label_encoding_train)\nprediction = np.expm1(loovc_lgb_model.predict(label_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('Label_LOOCV.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06f1fa9a01b891a81d0c2add584f3fbd555c18d2",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n#K-folds Validation function\nn_folds = 5 # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nfrequency_encoding_all = df_all.copy()\n    \ndef frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')\n\nfor col in categorical_features:\n    frequency_encoding_all = frequency_encoding(frequency_encoding_all, col)\nfrequency_encoding_all = frequency_encoding_all.drop(categorical_features,axis=1, inplace=False)\n\ndel frequency_encoding_all['SalePrice']\nfrequency_encoding_train = frequency_encoding_all[:len_train]\nfrequency_encoding_test = frequency_encoding_all[len_train:]\ndel frequency_encoding_all\n\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \n\nscore = rmsle_cv(model,frequency_encoding_train)  \nprint(\"Frequency encoding(5-folds) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nkfold_lgb_model = model.fit(frequency_encoding_train, y_reg)\ntrain_prediction = model.predict(frequency_encoding_train)\nprediction = np.expm1(model.predict(frequency_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('Frequency_5FOLD.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "796b90f46236de684d55383cb6b17e5ec4efcc42",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "#K-folds Validation function\nn_folds = frequency_encoding_train.shape[0] # if LOOCV k = df_train.shape[0]\n\ndef rmsle_cv(model,df):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_reg, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.05, n_estimators=720,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \nscore = rmsle_cv(model,frequency_encoding_train)  \nprint(\"Label encoding(LOOCV) LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nloovc_lgb_model = model.fit(frequency_encoding_train, y_reg)\ntrain_prediction = model.predict(frequency_encoding_train)\nprediction = np.expm1(loovc_lgb_model.predict(frequency_encoding_test.values))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': prediction})\nsubmission.to_csv('Frequency_LOOCV.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83077d04aafe04ecfc96008001b7fc56da65e7f8"
      },
      "cell_type": "markdown",
      "source": "|    | One-hot encoding(5-fold) | Label encoding(5-fold) | Frequency encoding(5-fold) |  One-hot encoding(LOOCV) | Label encoding(LOOCV) | Frequency encoding(LOOCV)|\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|\n| CV |0.1167(0.0069)                |0.1169(0.0055)                |0.1169 (0.0056)                    |0.0793(0.0850)               | 0.0795 (0.0853)             | 0.0799 (0.0843)             |\n| LB |                  |                |                    |               |              |              |"
    },
    {
      "metadata": {
        "_uuid": "3d5f579d94a416951865d2ee1cdaeb1c06672fd3"
      },
      "cell_type": "markdown",
      "source": "## 3. Feature Engineering\n### 3.1. Addition, subtraction, multiplication, division.\n|Variable Name    | Description |\n|----|------------------|\n| Total_sqr_footage  |BsmtFinSF1 + BsmtFinSF2 + 1stFlrSF + 2ndFlrSF|\n|Total_Bathrooms  | FullBath + 0.5xHalfBath + BsmtFullBath + 0.5xBsmtHalfBath                | \n|Total_porch_sf |OpenPorchSF + 3SsnPorch + EnclosedPorch + ScreenPorch + WoodDeckSF|\n|TotalHouse |TotalBsmtSF+1stFlrSF+2ndFlrSF|\n|TotalArea |TotalBsmtSF+1stFlrSF+2ndFlrSF+GarageArea|\n|GrLivArea_OverallQual|GrLivArea * OverallQual|\n|LotArea_OverallQual|LotArea * OverallQual|"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb7b0e28adb0850059a80639c58342530ff75c31"
      },
      "cell_type": "code",
      "source": "one_hot_encoding = pd.concat([one_hot_encoding_train,one_hot_encoding_test])\none_hot_encoding['Total_sqr_footage'] = one_hot_encoding['BsmtFinSF1'] + one_hot_encoding['BsmtFinSF2'] + one_hot_encoding['1stFlrSF'] + one_hot_encoding['2ndFlrSF']\none_hot_encoding['Total_Bathrooms'] = one_hot_encoding['FullBath'] + one_hot_encoding['HalfBath'] + one_hot_encoding['BsmtFullBath'] + one_hot_encoding['BsmtHalfBath']\none_hot_encoding['Total_porch_sf'] = one_hot_encoding['OpenPorchSF'] + one_hot_encoding['3SsnPorch'] + one_hot_encoding['EnclosedPorch'] + one_hot_encoding['ScreenPorch'] + one_hot_encoding['WoodDeckSF'] \none_hot_encoding['TotalHouse'] = one_hot_encoding['TotalBsmtSF'] + one_hot_encoding['1stFlrSF'] + one_hot_encoding['2ndFlrSF']\none_hot_encoding['TotalArea'] = one_hot_encoding['TotalBsmtSF'] + one_hot_encoding['1stFlrSF'] + one_hot_encoding['2ndFlrSF'] + one_hot_encoding[\"GarageArea\"]\none_hot_encoding['GrLivArea_OverallQual'] = one_hot_encoding['GrLivArea'] * one_hot_encoding['OverallQual']\none_hot_encoding['LotArea_OverallQual'] = one_hot_encoding['LotArea'] * one_hot_encoding['OverallQual']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3ac88a55370feb6e7b37fc90f34eb16b9c49543"
      },
      "cell_type": "markdown",
      "source": "### 3.2. Statistical variables. Ex) min, max, mean, std, skewness, kurtosis etc...\n|Variable Name    | Description |\n|----|------------------|\n| overall_statistical  | The statistical value of a person with a house of similar **quality** to me.|\n| neighborhood_statistical  | The statistical value of a person with a house of similar **region** to me.               | "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09592af00c83c3a6f1846676274a1e571e380b87"
      },
      "cell_type": "code",
      "source": "#one_hot_encoding['SalePrice'] = y_reg\n#agg = one_hot_encoding.groupby(['OverallQual'])['SalePrice'].agg('mean').reset_index()\n#one_hot_encoding = one_hot_encoding.merge(agg, suffixes=[\"\", \"_mean\"], how='left', on=['OverallQual'])\n#agg = one_hot_encoding.groupby(['neighborhood'])['SalePrice'].agg('mean').reset_index()\n#one_hot_encoding = one_hot_encoding.merge(agg, suffixes=[\"\", \"_mean\"], how='left', on=['OverallQual'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "29bbb99823315929e7a5c8d4e0a7e9149faef9a4"
      },
      "cell_type": "markdown",
      "source": "However, this is done before encoding, and if you apply this variable, the score has dropped so i don't use it"
    },
    {
      "metadata": {
        "_uuid": "b745f25b8730e135a19569669535d90b24748374"
      },
      "cell_type": "markdown",
      "source": "### 3.3. Make a Categorical Variable. 1 if exist, 0 if absent\n|Variable Name    | Description |\n|----|------------------|\n| haspool  |  1 if pool exist, else 0 |\n| hasgarage  | 1 if garage exist, else 0 | \n| hasbsmt  |  1 if bsmt exist, else 0 |\n| hasfireplace  | 1 if fireplace exist, else 0  | "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ec40506e0bb8ed243386c0a0efedce3d6f9cd99"
      },
      "cell_type": "code",
      "source": "one_hot_encoding['haspool'] = one_hot_encoding['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\none_hot_encoding['hasgarage'] = one_hot_encoding['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\none_hot_encoding['hasbsmt'] = one_hot_encoding['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\none_hot_encoding['hasfireplace'] = one_hot_encoding['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a502891a3bc550da73c3b17781ace587b390dd9"
      },
      "cell_type": "markdown",
      "source": "### 3.4. Normalization "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d6d41fcb0ead21dec57f741b65a9e52efc6d99e"
      },
      "cell_type": "code",
      "source": "from scipy.stats import skew\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in one_hot_encoding.columns:\n    if one_hot_encoding[i].dtype in numeric_dtypes: \n        numerics2.append(i)\n\nskew_features = one_hot_encoding[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})\nskews.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54abd0272be40c905cb1f53fe914f8ca9949065a"
      },
      "cell_type": "code",
      "source": "from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skew_features[skew_features > 0.5]\nhigh_skew = high_skew\nskew_index = high_skew.index\n\nfor i in skew_index:\n    one_hot_encoding[i]= boxcox1p(one_hot_encoding[i], boxcox_normmax(one_hot_encoding[i]+1))\n\n        \nskew_features2 = one_hot_encoding[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews2 = pd.DataFrame({'skew':skew_features2})\nskews2.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a30d9d720ea2a758448e7a5939dcc39c8f045b5"
      },
      "cell_type": "markdown",
      "source": "## 4. Modeling\nI refer model code & ensemble code in [Ming - Top 2% from Laurenstc on house price prediction\n](https://www.kaggle.com/hemingwei/top-2-from-laurenstc-on-house-price-prediction/notebook)"
    },
    {
      "metadata": {
        "_uuid": "65be296e099a560c351d03254082765214064aa3"
      },
      "cell_type": "markdown",
      "source": "### 4.1. Ridge"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e84c3a4e4dc84ddf1656bebf389360d90bf0b542"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import RidgeCV\n\none_hot_encoding_train = one_hot_encoding[:len_train]\none_hot_encoding_test = one_hot_encoding[len_train:]\ndel one_hot_encoding\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, one_hot_encoding_train, y_reg, \n                                   scoring=\"neg_mean_squared_error\", \n                                   cv = kfolds))\n    return(rmse)\n\ndef ridge_selector(k):\n    ridge_model = make_pipeline(RobustScaler(),\n                                RidgeCV(alphas = [k],\n                                        cv=kfolds)).fit(one_hot_encoding_train, y_reg)\n    \n    ridge_rmse = cv_rmse(ridge_model).mean()\n    return(ridge_rmse)\n\nr_alphas = [.0001, .0003, .0005, .0007, .0009, \n          .01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 60, 70, 80]\n\nridge_scores = []\nfor alpha in r_alphas:\n    score = ridge_selector(alpha)\n    ridge_scores.append(score)\n    \nplt.plot(r_alphas, ridge_scores, label='Ridge')\nplt.legend('center')\nplt.xlabel('alpha')\nplt.ylabel('score')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7f77f39089a29f6a4a2f9bdd59d7bc57bf2aad5b"
      },
      "cell_type": "markdown",
      "source": "Check more accuracy alpha"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e8ddce4c9f3418b98a6836e2fdab00eddee3380"
      },
      "cell_type": "code",
      "source": "alphas_alt = [9,9.1,9.2,9.3,9.4,9.5,9.6,9.7,9.8,9.9,10,10.1,10.2]\n\nridge_model2 = make_pipeline(RobustScaler(),\n                            RidgeCV(alphas = alphas_alt,\n                                    cv=kfolds)).fit(one_hot_encoding_train, y_reg)\n\nprint(\"Ridge rmse : \",cv_rmse(ridge_model2).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d90a59cdaf9e4abde72ac9c30a7625b04d9414d"
      },
      "cell_type": "code",
      "source": "print(\"Best of alpha in ridge model :\" ,ridge_model2.steps[1][1].alpha_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b3ca4a737d6bfbb267f35028a265c3d317bc581"
      },
      "cell_type": "code",
      "source": "ridge_coef = pd.DataFrame(np.round_(ridge_model2.steps[1][1].coef_, decimals=3), \none_hot_encoding_test.columns, columns = [\"penalized_regression_coefficients\"])\n# remove the non-zero coefficients\nridge_coef = ridge_coef[ridge_coef['penalized_regression_coefficients'] != 0]\n# sort the values from high to low\nridge_coef = ridge_coef.sort_values(by = 'penalized_regression_coefficients', \nascending = False)\n\n# plot the sorted dataframe\nfig = plt.figure(figsize = (25,25))\nax = sns.barplot(x = 'penalized_regression_coefficients', y= ridge_coef.index , \ndata=ridge_coef)\nax.set(xlabel='Penalized Regression Coefficients')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52610296a212f8ec0c67b85ca588765ab9e71d93"
      },
      "cell_type": "markdown",
      "source": "### 4.2 Lasso "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9949cb6a8d483a316dab8e1e797d47ea2985f024"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LassoCV\n\nalphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n           0.0006, 0.0007, 0.0008]\n\n\nlasso_model2 = make_pipeline(RobustScaler(),\n                             LassoCV(max_iter=1e7,\n                                    alphas = alphas2,\n                                    random_state = 42)).fit(one_hot_encoding_train, y_reg)\nscores = lasso_model2.steps[1][1].mse_path_\n\nplt.plot(alphas2, scores, label='Lasso')\nplt.legend(loc='center')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "083b8b38499383800ac13bd2b5aa04597e023369"
      },
      "cell_type": "code",
      "source": "print(\"Best of alpha in lasso model :\",lasso_model2.steps[1][1].alpha_)\nprint(\"lasso rmse : \",cv_rmse(lasso_model2).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80d3f63187cb9fc0744ad8f109ff482fd181d38b"
      },
      "cell_type": "code",
      "source": "lasso_coef = pd.DataFrame(np.round_(lasso_model2.steps[1][1].coef_, decimals=3), \none_hot_encoding_test.columns, columns = [\"penalized_regression_coefficients\"])\n# remove the non-zero coefficients\nlasso_coef = lasso_coef[lasso_coef['penalized_regression_coefficients'] != 0]\n# sort the values from high to low\nlasso_coef = lasso_coef.sort_values(by = 'penalized_regression_coefficients', \nascending = False)\n\n# plot the sorted dataframe\nfig = plt.figure(figsize = (25,25))\nax = sns.barplot(x = 'penalized_regression_coefficients', y= lasso_coef.index , \ndata=lasso_coef)\nax.set(xlabel='Penalized Regression Coefficients')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a3a9a062df174259e9fb6f3f627108ac4ed3104"
      },
      "cell_type": "markdown",
      "source": "### 4.3 ElasticNetCV"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "498f9f182af2653e2f1444935ea931d913d592a4"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import ElasticNetCV\n\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas, \n                                        cv=kfolds, l1_ratio=e_l1ratio))\n\nelastic_model3 = elastic_cv.fit(one_hot_encoding_train, y_reg)\nprint(\"elastic model rmse : \",cv_rmse(elastic_model3).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "667c735cfb77cf603bb17e62eeeb65b7216b4937"
      },
      "cell_type": "markdown",
      "source": "### 4.4 XGBoost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c56bd47b604e7b1f39e5a2fc47db45e8a9394e1"
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\nfrom xgboost import XGBRegressor\n\nxgb3 = XGBRegressor(learning_rate =0.01, n_estimators=5200, max_depth=3,\n                     min_child_weight=0 ,gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,objective= 'reg:linear',\n                     nthread=4,scale_pos_weight=1,seed=27, reg_alpha=0.00006)\n\nxgb_fit = xgb3.fit(one_hot_encoding_train, y_reg)\nprint(\"Xgboost model rmse : \",cv_rmse(xgb_fit).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58b82caf72798ba1e2674d98a1aaa20645a1065d"
      },
      "cell_type": "markdown",
      "source": "### 4.5 LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d49edbfe1be2f8274b0d0fdd4417b3df2da0af3"
      },
      "cell_type": "code",
      "source": "model = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.01, n_estimators=5000,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \n\nlgbm_fit = model.fit(one_hot_encoding_train, y_reg)\nprint(\"lightgbm model rmse : \",cv_rmse(model).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "033997d36d8c19b1e60d0738eaae9612d2bba3d0"
      },
      "cell_type": "code",
      "source": "feature_importance_df = pd.DataFrame()\nfeature_importance_df[\"feature\"] = one_hot_encoding_test.columns\nfeature_importance_df[\"importance\"] = model.feature_importances_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01556a856d3a52e3ba983b74ef9725d158334fde"
      },
      "cell_type": "code",
      "source": "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "209ffa14d8b3197c9a477563bee14d312e1a6014"
      },
      "cell_type": "markdown",
      "source": "## 5. Ensemble\n### 5.1 Stacking\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Stacking.PNG?raw=true)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9042406783907b6ba06df7188913633425277faa"
      },
      "cell_type": "code",
      "source": "train_ridge_preds = ridge_model2.predict(one_hot_encoding_train)\ntrain_lasso_preds = lasso_model2.predict(one_hot_encoding_train)\ntrain_elastic_preds = elastic_model3.predict(one_hot_encoding_train)\ntrain_xgb_preds = xgb_fit.predict(one_hot_encoding_train)\ntrain_lgbm_preds = lgbm_fit.predict(one_hot_encoding_train)\n\nstackX = pd.DataFrame(np.transpose(np.array([train_ridge_preds,train_lasso_preds,train_elastic_preds,train_xgb_preds,train_lgbm_preds])))\n\nmodel = lgb.LGBMRegressor(objective='regression',num_leaves=5,  \n                              learning_rate=0.01, n_estimators=4500,  \n                              max_bin = 55, bagging_fraction = 0.8,  \n                              bagging_freq = 5, feature_fraction = 0.2319,  \n                              feature_fraction_seed=9, bagging_seed=9,  \n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  \n\nstack_fit = model.fit(stackX, y_reg)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb9397a37a1eec4e7e8f0e576b8e11507d9c1106"
      },
      "cell_type": "code",
      "source": "print(\"lightgbm model rmse : \",cv_rmse(model).mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02d3f87510f1404c52d813f6ed91cf6f36aa51e7"
      },
      "cell_type": "code",
      "source": "test_ridge_preds = ridge_model2.predict(one_hot_encoding_test)\ntest_lasso_preds = lasso_model2.predict(one_hot_encoding_test)\ntest_elastic_preds = elastic_model3.predict(one_hot_encoding_test)\ntest_xgb_preds = xgb_fit.predict(one_hot_encoding_test)\ntest_lgbm_preds = lgbm_fit.predict(one_hot_encoding_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "041e89af5d8d9f5c434a04af3f9264483eddd557"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame({'Id': Id, 'SalePrice': np.expm1(test_ridge_preds)})\nsubmission.to_csv('ridge.csv', index=False)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': np.expm1(test_lasso_preds)})\nsubmission.to_csv('lasso.csv', index=False)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': np.expm1(test_elastic_preds)})\nsubmission.to_csv('elastic.csv', index=False)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': np.expm1(test_xgb_preds)})\nsubmission.to_csv('xgb.csv', index=False)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': np.expm1(test_lgbm_preds)})\nsubmission.to_csv('lgbm.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf1301d51f4f96d57cbfd91258b5ce36b6cbf697"
      },
      "cell_type": "code",
      "source": "test_stackX = pd.DataFrame(np.transpose(np.array([test_ridge_preds,test_lasso_preds,test_elastic_preds,test_xgb_preds,test_lgbm_preds])))\nstack_pred = np.expm1(stack_fit.predict(test_stackX))\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': stack_pred})\nsubmission.to_csv('LGB_stacking.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51456933f7263d356ea977c6d3d909e2fe70626b"
      },
      "cell_type": "markdown",
      "source": "Before apply boxcox(Version18)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11352                |0.11225                |0.11244                   |0.11467              |  0.11524            |             |             |             | \n| LB | 0.12282                | 0.12490               |0.12503                   | 0.12762              | 0.12098             |0.12595             |             |             | \n\nAfter apply boxcox(Version19)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11082               |0.11044              | 0.11028                  |0.11327              |  0..11517            |             |             |             | \n| LB | 0.12158                | 0.12454               |0.12457                   | 0.12827              | 0.12183             |             |             |             | "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1633d290802ffba65691e9480d98839becde4c8e"
      },
      "cell_type": "markdown",
      "source": "### 5.2 Average\n![](https://github.com/choco9966/Team-EDA/blob/master/image/Average1.PNG?raw=true)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5520b8c7be595512b40f6a67d572eaa179f0d86"
      },
      "cell_type": "code",
      "source": "### Simple Average\naverage = 0.2*test_ridge_preds + 0.2*test_lasso_preds + 0.2*test_elastic_preds + 0.2*test_xgb_preds + 0.2*test_lgbm_preds\naverage = np.expm1(average)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': average})\nsubmission.to_csv('SimpleAvg.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c0a7fd0873d239d8e7b254d6f21ac846ce3fa848"
      },
      "cell_type": "markdown",
      "source": "Before apply boxcox(Version18)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11352                |0.11225                |0.11244                   |0.11467              |  0.11524            |             |             |             | \n| LB | 0.12282                | 0.12490               |0.12503                   | 0.12762              | 0.12098             |0.12595             | 0.12110            |             | \n\nAfter apply boxcox(Version19)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11082               |0.11044              | 0.11028                  |0.11327              |  0..11517            |             |             |             | \n| LB | 0.12282                | 0.12490               |0.12503                   | 0.12762              | 0.12098             |0.12595             |             |             | "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "772dc252858d18f30812d69ca7354bd65d35cf79"
      },
      "cell_type": "code",
      "source": "### Weight Average\nweight_average1 = 0.3980767*test_ridge_preds + -0.03065248*test_lasso_preds + 0.12292276*test_elastic_preds + -0.20778906*test_xgb_preds + 0.71743772*test_lgbm_preds\nweight_average1 = np.expm1(weight_average1)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': weight_average1})\nsubmission.to_csv('WeightAvg1.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fad6dd2f7bc2f85ba91d6144f6f627f50b3cfba8"
      },
      "cell_type": "code",
      "source": "### Weight Average\nweight_average2 = 0.28018685*test_ridge_preds + 0.10355858*test_lasso_preds + 0.1161906*test_elastic_preds + 0.006204816*test_xgb_preds + 0.49386202*test_lgbm_preds\nweight_average2 = np.expm1(weight_average2)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': weight_average2})\nsubmission.to_csv('WeightAvg2.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "721db0395968dc23db9fc3bc8465a4e68ae53489"
      },
      "cell_type": "markdown",
      "source": "Before apply boxcox(Version18)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11352                |0.11225                |0.11244                   |0.11467              |  0.11524            |             |             |             | \n| LB | 0.12282                | 0.12490               |0.12503                   | 0.12762              | 0.12098             |0.12595             | 0.12110            |   0.11883          | \n\nAfter apply boxcox(Version19)\n\n|    | Ridge | Lasso |  Elastic |  Xgboost | LightGBM | Stacking  |Simple Average            | Weight Average\n|----|------------------|----------------|--------------------|---------------|--------------|--------------|--------------|--------------|--------------|\n| CV |0.11082               |0.11044              | 0.11028                  |0.11327              |  0..11517            |             |             |             | \n| LB | 0.12282                | 0.12490               |0.12503                   | 0.12762              | 0.12098             |0.12595             |             |             | "
    },
    {
      "metadata": {
        "_uuid": "459cc801b08124007b1931db863b5d4cc458e21d"
      },
      "cell_type": "markdown",
      "source": "outlier in https://www.kaggle.com/agehsbarg/top-10-0-10943-stacking-mice-and-brutal-force "
    },
    {
      "metadata": {
        "_uuid": "ecdcca2e29dd2d7803d5c91eacc1f3a1a1d4db47"
      },
      "cell_type": "markdown",
      "source": "## 6. Next \n\nIn addition, you must continue with EDA. \n- Find a new feature and outliers.\n- Tuning the hyperparameters.\n- Read a other's Kernels\n- Ensemble with other's Kernel output\n\nthen, you can get a Nice Score!!!\n\n![](https://github.com/choco9966/Team-EDA/blob/master/image/last_Score.PNG?raw=true)\n\nThank you for reading it."
    },
    {
      "metadata": {
        "_uuid": "040312277d94b66e9291a8df28bf38301e0ed3ed"
      },
      "cell_type": "markdown",
      "source": "### Resources\nhttps://www.slideshare.net/yeonminkim/pycon-korea-2018-kaggle-tutorialkaggle-break  \nKaggle 우승작으로 배우는 머신러닝 탐구생활   \nhttps://www.kaggle.com/nextbigwhat/eda-for-categorical-variables-a-beginner-s-way    \nhttps://www.kaggle.com/jens0306/easy-prediction-using-lightgbm-model    \nhttps://www.kaggle.com/hemingwei/top-2-from-laurenstc-on-house-price-prediction/notebook    \n  "
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}